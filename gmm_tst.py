# -*- coding: utf-8 -*-
"""gmm_tst.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qZqWHxGUoWY_rXK5BYNfnCbYjDcf91hh

## B∆∞·ªõc 1: Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.mixture import GaussianMixture
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# Load d·ªØ li·ªáu t·ª´ CSV
df = pd.read_csv("/content/train.csv")

# ƒê·∫∑c tr∆∞ng v√† nh√£n
X = df.drop(columns=["Activity", "subject"])
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# M√£ h√≥a nh√£n Activity
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(df["Activity"])

X.dropna()

"""## Gaussian Mixture Model"""

gmm = GaussianMixture(n_components=4, random_state=42)
cluster_ids = gmm.fit_predict(X_scaled)

# One-hot encoding c·ª•m
cluster_onehot = OneHotEncoder(sparse_output=False).fit_transform(cluster_ids.reshape(-1, 1))

# T·∫°o d·ªØ li·ªáu t·ªïng h·ª£p
X_combined = np.hstack([X_scaled, cluster_onehot])
df["ClusterID"] = cluster_ids
df["ActivityEncoded"] = y_encoded

"""## B∆∞·ªõc 3: Hu·∫•n luy·ªán Transformer ri√™ng cho t·ª´ng c·ª•m"""

class SensorDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.long)
    def __len__(self):
        return len(self.X)
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

class SimpleTST(nn.Module):
    def __init__(self, input_dim, n_classes):
        super().__init__()
        self.linear_proj = nn.Linear(input_dim, 128)
        self.encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8, batch_first=True)
        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=2)
        self.classifier = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, n_classes)
        )
    def forward(self, x):
        x = self.linear_proj(x).unsqueeze(1)
        x = self.transformer(x).squeeze(1)
        return self.classifier(x)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
all_preds, all_labels, all_clusters = [], [], []

for cluster_id in np.unique(cluster_ids):
    print(f"\nüîÑ Training for Cluster {cluster_id}")
    idx = cluster_ids == cluster_id
    X_cluster = X_combined[idx]
    y_cluster = y_encoded[idx]

    unique_classes = np.unique(y_cluster)
    if len(unique_classes) == 1:
        print(f"‚ö†Ô∏è Cluster {cluster_id} ch·ªâ c√≥ m·ªôt class ({label_encoder.inverse_transform(unique_classes)[0]}), b·ªè hu·∫•n luy·ªán.")
        all_preds.extend(y_cluster)
        all_labels.extend(y_cluster)
        all_clusters.extend([cluster_id] * len(y_cluster))
        continue

    # N·∫øu s·ªë m·∫´u nh·ªè kh√¥ng ƒë·ªß stratify, fallback
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X_cluster, y_cluster, test_size=0.2, stratify=y_cluster, random_state=42
        )
    except ValueError:
        print(f"‚ö†Ô∏è Cluster {cluster_id} c√≥ class qu√° √≠t, b·ªè stratify.")
        X_train, X_test, y_train, y_test = train_test_split(
            X_cluster, y_cluster, test_size=0.2, random_state=42
        )

    train_loader = DataLoader(SensorDataset(X_train, y_train), batch_size=64, shuffle=True)
    test_loader = DataLoader(SensorDataset(X_test, y_test), batch_size=64)

    model = SimpleTST(input_dim=X_combined.shape[1], n_classes=len(np.unique(y_encoded))).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    criterion = nn.CrossEntropyLoss()

    for epoch in range(10):
        model.train()
        total_loss = 0
        for xb, yb in train_loader:
            xb, yb = xb.to(device), yb.to(device)
            pred = model(xb)
            loss = criterion(pred, yb)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"üìâ Epoch {epoch+1}, Loss: {total_loss:.4f}")

    # ƒê√°nh gi√°
    model.eval()
    cluster_preds, cluster_labels = [], []
    with torch.no_grad():
        for xb, yb in test_loader:
            xb = xb.to(device)
            pred = model(xb)
            pred_labels = pred.argmax(dim=1).cpu().numpy()
            cluster_preds.extend(pred_labels)
            cluster_labels.extend(yb.numpy())

    all_preds.extend(cluster_preds)
    all_labels.extend(cluster_labels)
    all_clusters.extend([cluster_id] * len(cluster_labels))

"""## B∆∞·ªõc 4: ƒê√°nh gi√° m√¥ h√¨nh"""

from sklearn.metrics import classification_report, accuracy_score, f1_score


print("Accuracy:", accuracy_score(all_labels, all_preds))


print("\nüîç Classification Report:")
print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))

# Ph√¢n t√≠ch theo c·ª•m
df_results = pd.DataFrame({
    "Cluster": all_clusters,
    "TrueLabel": label_encoder.inverse_transform(all_labels),
    "PredictedLabel": label_encoder.inverse_transform(all_preds)
})


print(df_results.groupby("Cluster").size())


print(df_results)